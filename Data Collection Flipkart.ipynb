{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdfabe5-23d4-429f-ab1a-0e962dedb0e0",
   "metadata": {},
   "source": [
    "# 1. Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5048f327-271e-40a9-9fd7-a34501f7b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921e0c1-8401-42e6-9d67-d3323b6936b1",
   "metadata": {},
   "source": [
    "## Explanation for Modules\n",
    "- `requests` module is to send a request to the URL to fetch the data.\n",
    "- `BeautifulSoup` is a class using the object of which we will deal with the scraped HTML data.\n",
    "- `re` is for using regex patterns to filter out data and create our dataframe in an organized format.\n",
    "- `numpy` and `pandas` module if for manipulating data values and handling the data overall.\n",
    "- `time` module is used to create a time delay during scraping.\n",
    "- `random` module is used to generate random numbers to be used during scraping time delays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390ae5d1-85d1-4dc1-bd35-142aecb9cdf7",
   "metadata": {},
   "source": [
    "# Scraping All The Webpages Of Flipkart For Laptop Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4927b94-29f4-46a7-be97-86c13319413b",
   "metadata": {},
   "source": [
    "<h2><a href=\"https://www.flipkart.com/search?q=laptop&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=1\">Click here for the Initial Site</a></h2>\n",
    "\n",
    "![image.jpg](images\\pages.jpg)\n",
    "\n",
    "- As highlighted in the red box of the above image we have access to **68 pages** of flipkart to scrape the data from.\n",
    "- So based on that we will write the code to scrape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be564835-e650-42a8-877d-b26cf1d17326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Request Headers to scrape the data\n",
    "request_header = {\n",
    "    'Content-Type': 'text/html; charset=UTF-8',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Referer': 'https://www.flipkart.com/',\n",
    "    'Origin': 'https://www.flipkart.com',\n",
    "    'Accept-Language': 'en-US,en;q=0.9'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b29885-7ca1-487d-99e8-8528cc0f5d13",
   "metadata": {},
   "source": [
    "### Understanding The Request Headers\n",
    "\n",
    "- **Content-Type**: This tells the server what kind of data you’re sending. In this case, it’s HTML text with a specific character set (UTF-8).\n",
    "- **User-Agent**: This identifies the browser and operating system making the request. It helps the server understand how to format the response. For example, we are using Firefox on a Windows 10 machine.\n",
    "- **Accept-Encoding**: This tells the server which compression methods your client can handle. Here, it indicates that the client can accept responses compressed with gzip, deflate, or br (Brotli).\n",
    "- **Referer**: This indicates the URL from which the request originated. It helps the server understand the context of the request. In this scenario, it shows that the request is coming from Flipkart’s website.\n",
    "- **Origin**: Similar to Referer, this specifies the origin of the request, which is also Flipkart in this case. It’s used for security purposes, particularly with cross-origin requests.\n",
    "- **Accept-Language**: This tells the server which languages your client prefers. Here, it indicates a preference for US English, but can also accept other forms of English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e75b9c5f-6e33-42b7-9e19-93b1e4075b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the URL as a f-string\n",
    "page = 1\n",
    "URL = f\"https://www.flipkart.com/search?q=laptop&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page={page}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3070d5-3167-41bb-8a6c-520d506f64a7",
   "metadata": {},
   "source": [
    "- The URL is stored as f-string becuase, by changing the page number in the URL, we can access the next page data, this can be utilized in conjunction with for loop to scrape all the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c40690-24d5-4f43-b723-0e97e9e56ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Delay: 8 seconds    : Page 1  status: <Response [200]>\n",
      "Time Delay: 10 seconds    : Page 2  status: <Response [200]>\n",
      "Time Delay: 9 seconds    : Page 3  status: <Response [200]>\n",
      "Time Delay: 10 seconds    : Page 4  status: <Response [200]>\n",
      "Time Delay: 9 seconds    : Page 5  status: <Response [200]>\n",
      "Time Delay: 8 seconds    : Page 6  status: <Response [200]>\n",
      "Time Delay: 5 seconds    : Page 7  status: <Response [200]>\n",
      "Time Delay: 7 seconds    : Page 8  status: <Response [200]>\n",
      "Time Delay: 8 seconds    : Page 9  status: <Response [200]>\n",
      "Time Delay: 5 seconds    : Page 10  status: <Response [200]>\n",
      "Time Delay: 10 seconds    : Page 11  status: <Response [200]>\n",
      "Time Delay: 9 seconds    : Page 12  status: <Response [200]>\n",
      "Time Delay: 8 seconds    : Page 13  status: <Response [200]>\n",
      "Time Delay: 6 seconds    : Page 14  status: <Response [200]>\n",
      "Time Delay: 9 seconds    : Page 15  status: <Response [200]>\n",
      "Time Delay: 6 seconds    : Page 16  status: <Response [200]>\n",
      "Time Delay: 9 seconds    : Page 17  status: <Response [200]>\n",
      "Time Delay: 9 seconds    : Page 18  status: <Response [200]>\n",
      "Time Delay: 9 seconds    : Page 19  status: <Response [200]>\n",
      "Time Delay: 10 seconds    : Page 20  status: <Response [200]>\n",
      "Time Delay: 10 seconds    : Page 21  status: <Response [200]>\n",
      "Time Delay: 8 seconds    : Page 22  status: <Response [200]>\n",
      "Time Delay: 6 seconds    : Page 23  status: <Response [200]>\n",
      "Time Delay: 8 seconds    : Page 24  status: <Response [200]>\n",
      "Time Delay: 10 seconds    : Page 25  status: <Response [200]>\n",
      "Time Delay: 6 seconds    : Page 26  status: <Response [200]>\n",
      "Time Delay: 6 seconds    : Page 27  status: <Response [200]>\n",
      "Time Delay: 9 seconds    : Page 28  status: <Response [200]>\n",
      "Time Delay: 8 seconds    : Page 29  status: <Response [200]>\n",
      "Time Delay: 7 seconds    : Page 30  status: <Response [200]>\n",
      "Time Delay: 7 seconds    : Page 31  status: <Response [200]>\n",
      "Time Delay: 10 seconds    : Page 32  status: <Response [200]>\n",
      "Time Delay: 10 seconds    : Page 33  status: <Response [200]>\n",
      "Time Delay: 8 seconds    : Page 34  status: <Response [200]>\n",
      "Time Delay: 7 seconds    : Page 35  status: <Response [200]>\n",
      "Time Delay: 8 seconds    : Page 36  status: <Response [200]>\n",
      "Time Delay: 5 seconds    : Page 37  status: <Response [200]>\n",
      "Time Delay: 7 seconds    : Page 38  status: <Response [200]>\n",
      "Time Delay: 5 seconds    : Page 39  status: <Response [200]>\n",
      "Time Delay: 7 seconds    : Page 40  status: <Response [200]>\n",
      "Time Delay: 5 seconds    : Page 41  status: <Response [200]>\n",
      "Time Delay: 6 seconds    : Page 42  status: <Response [200]>\n",
      "Time Delay: 6 seconds    : Page 43  status: <Response [200]>\n",
      "Time Delay: 8 seconds    : Page 44  status: <Response [200]>\n",
      "Time Delay: 5 seconds    : Page 45  status: <Response [200]>\n",
      "Time Delay: 9 seconds    : Page 46  status: <Response [200]>\n",
      "Time Delay: 10 seconds    : Page 47  status: <Response [200]>\n",
      "Time Delay: 10 seconds    : Page 48  status: <Response [200]>\n",
      "Time Delay: 5 seconds    : Page 49  status: <Response [200]>\n",
      "Time Delay: 9 seconds    : Page 50  status: <Response [200]>\n",
      "Time Delay: 5 seconds    : Page 51  status: <Response [200]>\n",
      "Time Delay: 5 seconds    : Page 52  status: <Response [200]>\n",
      "Time Delay: 9 seconds    : Page 53  status: <Response [200]>\n",
      "Time Delay: 8 seconds    : Page 54  status: <Response [200]>\n",
      "Time Delay: 8 seconds    : Page 55  status: <Response [200]>\n",
      "Time Delay: 7 seconds    : Page 56  status: <Response [200]>\n",
      "Time Delay: 8 seconds    : Page 57  status: <Response [200]>\n",
      "Time Delay: 5 seconds    : Page 58  status: <Response [200]>\n",
      "Time Delay: 6 seconds    : Page 59  status: <Response [200]>\n",
      "Time Delay: 9 seconds    : Page 60  status: <Response [200]>\n",
      "Time Delay: 9 seconds    : Page 61  status: <Response [200]>\n",
      "Time Delay: 6 seconds    : Page 62  status: <Response [200]>\n",
      "Time Delay: 9 seconds    : Page 63  status: <Response [200]>\n",
      "Time Delay: 8 seconds    : Page 64  status: <Response [200]>\n",
      "Time Delay: 7 seconds    : Page 65  status: <Response [200]>\n",
      "Time Delay: 5 seconds    : Page 66  status: <Response [200]>\n",
      "Time Delay: 9 seconds    : Page 67  status: <Response [200]>\n",
      "Time Delay: 5 seconds    : Page 68  status: <Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# Scraping Code\n",
    "\n",
    "total_pages = 68 # Total number of pages being scraped\n",
    "i = 1 # Counter to self verify the pages being scraped successfully\n",
    "raw_text = [] # List to store all the raw html code\n",
    "\n",
    "# Loop to iterate over all the pages by changing the f-string URL\n",
    "for page in range (1, total_pages+1):\n",
    "\n",
    "    # Fetching the data from URL based on the above request headers\n",
    "    response = requests.get(URL, headers=request_header)\n",
    "\n",
    "    # Random number to be used as time delay in order to make the script behaviour more human like\n",
    "    delay = random.randint(5,10)\n",
    "    print(\"Time Delay:\",delay,end=\" seconds    : \")\n",
    "\n",
    "    # While Loop: covers the edge case wherein the first attempt to fetch the data failed, \n",
    "    # by continuously requesting the data at irregular time intervals in order to mimic human behavior\n",
    "    while response.status_code!=200:\n",
    "        time.sleep(delay)\n",
    "        response = requests.get(URL,headers=request_header)\n",
    "\n",
    "    # Confirmation Message of Successful Scrape\n",
    "    print(\"Page\",i,\" status:\",response)\n",
    "\n",
    "    # Incrementing Page Counter\n",
    "    i+=1  \n",
    "\n",
    "    # Appending the raw HTML code in the list\n",
    "    raw_text.append(response.text)\n",
    "\n",
    "    # A random delay before requesting the data from next page\n",
    "    time.sleep(delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89782095-a5ac-4ba0-98f7-9f94bc166af9",
   "metadata": {},
   "source": [
    "# 3. Saving The Raw HTML Data in CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0302d0-0943-434e-94a8-64e2bb30a157",
   "metadata": {},
   "source": [
    "- Now we will save the raw HTML code for each page in a CSV by converting the list into a dataframe.\n",
    "- Saving in CSV will ensure that we don't have to scrape the entire data everytime we want to work on the data as scraping itself is a time consuming process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e490c331-2ddf-4c22-9079-5af29bd27b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Raw Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;link hre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;link hre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;link hre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;link hre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;link hre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Raw Data\n",
       "0  <!doctype html><html lang=\"en\"><head><link hre...\n",
       "1  <!doctype html><html lang=\"en\"><head><link hre...\n",
       "2  <!doctype html><html lang=\"en\"><head><link hre...\n",
       "3  <!doctype html><html lang=\"en\"><head><link hre...\n",
       "4  <!doctype html><html lang=\"en\"><head><link hre..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the list to Data Frame\n",
    "df = pd.DataFrame(raw_text,columns=[\"Raw Data\"])\n",
    "\n",
    "# Printing a sample to ensure correct data format\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99eaca52-ee93-4ab2-bdb9-c651f97ad35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe has been created successfully and can now be saved in a CSV file\n",
    "df.to_csv(r\"data\\raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec43efed-e691-45ec-8767-2ead10e0a846",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status\">HTML Error Codes</a>: Used for refrencing response codes and their meaning.\n",
    "2. <a href=\"https://docs.python-requests.org/en/latest/index.html\">Request Module of Python</a>: Used for creating custom headers during scraping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ubEnv",
   "language": "python",
   "name": "ubenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
